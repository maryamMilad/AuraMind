{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756aceae",
   "metadata": {
    "papermill": {
     "duration": 0.010319,
     "end_time": "2025-04-20T12:55:03.337727",
     "exception": false,
     "start_time": "2025-04-20T12:55:03.327408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📚 Table of Contents\n",
    "\n",
    "1. [Problem Statement](#ProblemStatement)\n",
    "2. [Solution](#Solution)\n",
    "3. [Introduction](#Introduction)\n",
    "4. [SDK](#SDK)\n",
    "5. [API key](#APIKEY)\n",
    "6. [ChromaDB](#ChromaDB)\n",
    "7. [Embedding Creation](#Embeddings)\n",
    "8. [Journal Entries](#JournalEntries)\n",
    "9. [Relevant Context(long context hostory)](#RelevantContext)\n",
    "10. [RAG chat](#RAGchat)\n",
    "11. [SYSTEMT PROMPT](#systemprompt)\n",
    "12. [Evaluation Model](#evaluationmodel)\n",
    "13. [Evaluation Criteria](#evaluationcriteria)\n",
    "14. [getting model response](#gettingmodelresponse)\n",
    "15. [Conversation History](#conversationhistory)\n",
    "16. [Contributions](#contributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799387c6",
   "metadata": {
    "papermill": {
     "duration": 0.00942,
     "end_time": "2025-04-20T12:55:03.356237",
     "exception": false,
     "start_time": "2025-04-20T12:55:03.346817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "Mental health is a critical aspect of overall well-being, yet many individuals struggle to access timely and personalized support. Traditional therapy, while effective, can be expensive, stigmatized, or inaccessible due to geographical and scheduling constraints. Moreover, mental health needs are continuous — users may require support in moments of emotional distress, reflection, or self-discovery outside of scheduled sessions.\n",
    "\n",
    "There is a growing need for a digital companion that can understand emotional states, remember past conversations, and provide grounded, empathetic, and context-aware support — all while ensuring user privacy and safety. \n",
    "\n",
    "The challenge lies in building an AI system that is not only conversational and intelligent but also emotionally sensitive, ethically sound, and capable of adapting to the user's mental health journey through personalized interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43231d4",
   "metadata": {
    "papermill": {
     "duration": 0.008575,
     "end_time": "2025-04-20T12:55:03.373849",
     "exception": false,
     "start_time": "2025-04-20T12:55:03.365274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Solution\n",
    "\n",
    "To address the need for accessible, personalized mental health support, we propose **AuraMind** — a RAG-based (Retrieval Augmented Generation) virtual mental health assistant powered by GenAI.\n",
    "\n",
    "AuraMind leverages cutting-edge capabilities such as journal-based memory retrieval, long-context understanding, and empathetic natural language processing. Users can engage in meaningful conversations, reflect on their emotions through journaling, receive personalized wellness exercises, and be gently guided toward professional resources when needed.\n",
    "\n",
    "The system uses:\n",
    "- **Journal Entry Storage & Retrieval** to ground responses in the user’s emotional history.\n",
    "- **Embeddings + ChromaDB** for efficient similarity search over past entries.\n",
    "- **Function Calling** for structured actions like exercise suggestion or therapist resource lookup.\n",
    "- **Long-Term Chat Memory** to maintain continuity across sessions.\n",
    "- **Grounding** via RAG to ensure responses are contextually relevant and safe.\n",
    "\n",
    "This AI assistant is designed to **complement** traditional mental health care — offering a private, always-available space for reflection, emotional insight, and gentle support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28237c4",
   "metadata": {
    "papermill": {
     "duration": 0.008548,
     "end_time": "2025-04-20T12:55:03.391209",
     "exception": false,
     "start_time": "2025-04-20T12:55:03.382661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Installing and importing libraries and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8b7078",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-20T12:55:03.410327Z",
     "iopub.status.busy": "2025-04-20T12:55:03.410019Z",
     "iopub.status.idle": "2025-04-20T12:55:05.331160Z",
     "shell.execute_reply": "2025-04-20T12:55:05.330164Z"
    },
    "papermill": {
     "duration": 1.933001,
     "end_time": "2025-04-20T12:55:05.332975",
     "exception": false,
     "start_time": "2025-04-20T12:55:03.399974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7578d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:55:05.352774Z",
     "iopub.status.busy": "2025-04-20T12:55:05.352062Z",
     "iopub.status.idle": "2025-04-20T12:55:49.209588Z",
     "shell.execute_reply": "2025-04-20T12:55:49.208430Z"
    },
    "papermill": {
     "duration": 43.869558,
     "end_time": "2025-04-20T12:55:49.211895",
     "exception": false,
     "start_time": "2025-04-20T12:55:05.342337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-generativeai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4613f94a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:55:49.235953Z",
     "iopub.status.busy": "2025-04-20T12:55:49.235561Z",
     "iopub.status.idle": "2025-04-20T12:55:55.574636Z",
     "shell.execute_reply": "2025-04-20T12:55:55.573438Z"
    },
    "papermill": {
     "duration": 6.352844,
     "end_time": "2025-04-20T12:55:55.576512",
     "exception": false,
     "start_time": "2025-04-20T12:55:49.223668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uuid\r\n",
      "  Downloading uuid-1.30.tar.gz (5.8 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: uuid\r\n",
      "  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for uuid: filename=uuid-1.30-py3-none-any.whl size=6478 sha256=43aa4a3847fd9399e0319aa86bb97125ce4eb891ef286b8c41fdc769be6bfa0a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e0/01/df/bd20df409bd81f8b99e6cd343c5f49731dc0a20eefefdafae0\r\n",
      "Successfully built uuid\r\n",
      "Installing collected packages: uuid\r\n",
      "Successfully installed uuid-1.30\r\n"
     ]
    }
   ],
   "source": [
    "!pip install uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a587c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:55:55.602503Z",
     "iopub.status.busy": "2025-04-20T12:55:55.602172Z",
     "iopub.status.idle": "2025-04-20T12:55:55.607320Z",
     "shell.execute_reply": "2025-04-20T12:55:55.606361Z"
    },
    "papermill": {
     "duration": 0.020425,
     "end_time": "2025-04-20T12:55:55.608894",
     "exception": false,
     "start_time": "2025-04-20T12:55:55.588469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0855e56e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:55:55.633357Z",
     "iopub.status.busy": "2025-04-20T12:55:55.633053Z",
     "iopub.status.idle": "2025-04-20T12:55:59.255407Z",
     "shell.execute_reply": "2025-04-20T12:55:59.254370Z"
    },
    "papermill": {
     "duration": 3.636633,
     "end_time": "2025-04-20T12:55:59.257187",
     "exception": false,
     "start_time": "2025-04-20T12:55:55.620554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import chromadb\n",
    "import textwrap\n",
    "import os\n",
    "import pypdf\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "import time\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc44f5",
   "metadata": {
    "papermill": {
     "duration": 0.010874,
     "end_time": "2025-04-20T12:55:59.279479",
     "exception": false,
     "start_time": "2025-04-20T12:55:59.268605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## API key\n",
    "\n",
    "\n",
    "Set up Google API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f191790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:55:59.303403Z",
     "iopub.status.busy": "2025-04-20T12:55:59.302848Z",
     "iopub.status.idle": "2025-04-20T12:55:59.555328Z",
     "shell.execute_reply": "2025-04-20T12:55:59.554290Z"
    },
    "papermill": {
     "duration": 0.2665,
     "end_time": "2025-04-20T12:55:59.556882",
     "exception": false,
     "start_time": "2025-04-20T12:55:59.290382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API Key configured using Kaggle Secrets.\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "print(\"Gemini API Key configured using Kaggle Secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abaa845",
   "metadata": {
    "papermill": {
     "duration": 0.011231,
     "end_time": "2025-04-20T12:55:59.579258",
     "exception": false,
     "start_time": "2025-04-20T12:55:59.568027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Check that the Google API key is valid and list available Gemini models that support the generateContent method, which is needed for tasks like text generation, summarization, or chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88754f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:55:59.603582Z",
     "iopub.status.busy": "2025-04-20T12:55:59.603265Z",
     "iopub.status.idle": "2025-04-20T12:56:00.000497Z",
     "shell.execute_reply": "2025-04-20T12:55:59.999236Z"
    },
    "papermill": {
     "duration": 0.411651,
     "end_time": "2025-04-20T12:56:00.002276",
     "exception": false,
     "start_time": "2025-04-20T12:55:59.590625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available Gemini Models:\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"\\nAvailable Gemini Models:\")\n",
    "    for m in genai.list_models():\n",
    "        if 'generateContent' in m.supported_generation_methods:\n",
    "            print(m.name)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError checking models. Ensure your API key is configured correctly: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff140e",
   "metadata": {
    "papermill": {
     "duration": 0.010926,
     "end_time": "2025-04-20T12:56:00.024834",
     "exception": false,
     "start_time": "2025-04-20T12:56:00.013908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ChromaDB\n",
    "\n",
    "Gen AI Capability demonstrated: vector database (ChromaDB)\n",
    "\n",
    "Initialize a ChromaDB client and set up a collection named \"journal_entries1\" to store and retrieve the user's journal entries. The code ensures the collection exists—creating it if needed—and confirms it's ready by displaying the current number of items it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187ce5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:00.048754Z",
     "iopub.status.busy": "2025-04-20T12:56:00.048419Z",
     "iopub.status.idle": "2025-04-20T12:56:00.326042Z",
     "shell.execute_reply": "2025-04-20T12:56:00.324511Z"
    },
    "papermill": {
     "duration": 0.291688,
     "end_time": "2025-04-20T12:56:00.327615",
     "exception": false,
     "start_time": "2025-04-20T12:56:00.035927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB client...\n",
      "ChromaDB client initialized.\n",
      "Creating or getting ChromaDB collection: 'journal_entries1'...\n",
      "Collection 'journal_entries1' ready.\n",
      "Number of items currently in collection: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing ChromaDB client...\")\n",
    "client = chromadb.Client()\n",
    "print(\"ChromaDB client initialized.\")\n",
    "\n",
    "collection_name = \"journal_entries1\"\n",
    "print(f\"Creating or getting ChromaDB collection: '{collection_name}'...\")\n",
    "try:\n",
    "    journal_collection = client.get_or_create_collection(name=collection_name)\n",
    "    print(f\"Collection '{collection_name}' ready.\")\n",
    "    print(f\"Number of items currently in collection: {journal_collection.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating or getting collection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6225f92",
   "metadata": {
    "papermill": {
     "duration": 0.012705,
     "end_time": "2025-04-20T12:56:00.352510",
     "exception": false,
     "start_time": "2025-04-20T12:56:00.339805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Embedding Creation\n",
    "\n",
    "Gen AI Capability demonstrated: Embeddings\n",
    "\n",
    "This function defines embed_fn, which takes a piece of text and generates its embedding using Google’s text-embedding-004 model, designed for retrieval tasks. It first ensures the input is a non-empty string, then calls genai.embed_content with the appropriate model and task type. If the embedding is successfully generated, it returns the result; otherwise, it handles and prints any errors gracefully. This embedding function is essential for converting journal text into numerical vectors for storage and similarity search using ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675f04ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:00.377653Z",
     "iopub.status.busy": "2025-04-20T12:56:00.377377Z",
     "iopub.status.idle": "2025-04-20T12:56:00.383844Z",
     "shell.execute_reply": "2025-04-20T12:56:00.382605Z"
    },
    "papermill": {
     "duration": 0.020205,
     "end_time": "2025-04-20T12:56:00.385250",
     "exception": false,
     "start_time": "2025-04-20T12:56:00.365045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "embedding_model_name = 'models/text-embedding-004'\n",
    "\n",
    "def embed_fn(text_to_embed):\n",
    "    try:\n",
    "        if not isinstance(text_to_embed, str):\n",
    "             text_to_embed = str(text_to_embed)\n",
    "        if not text_to_embed.strip():\n",
    "            print(\"Warning: Attempted to embed an empty string. Returning None.\")\n",
    "            return None\n",
    "        result = genai.embed_content(\n",
    "            model=embedding_model_name,\n",
    "            content=text_to_embed,\n",
    "            task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "        )\n",
    "        return result['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text: '{text_to_embed[:50]}...'\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48715027",
   "metadata": {
    "papermill": {
     "duration": 0.011157,
     "end_time": "2025-04-20T12:56:00.476396",
     "exception": false,
     "start_time": "2025-04-20T12:56:00.465239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Testing embedding generation functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb66b4f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:00.500806Z",
     "iopub.status.busy": "2025-04-20T12:56:00.500458Z",
     "iopub.status.idle": "2025-04-20T12:56:01.140238Z",
     "shell.execute_reply": "2025-04-20T12:56:01.138904Z"
    },
    "papermill": {
     "duration": 0.654522,
     "end_time": "2025-04-20T12:56:01.142261",
     "exception": false,
     "start_time": "2025-04-20T12:56:00.487739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying embedding using sample_text: I want to drink coffee\n",
      "embedding successfull\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sample_text = 'I want to drink coffee'\n",
    "    print(f'trying embedding using sample_text: {sample_text}')\n",
    "    embedd = embed_fn(sample_text)\n",
    "    if embedd:\n",
    "        print('embedding successfull')\n",
    "        print(f'Embedding dimension: {len(embedd)}')\n",
    "    else:\n",
    "        print('failed to generate embedding')\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during the embedding function test: {e}\")\n",
    "    print(\"Please ensure your API key is correctly configured and the model name is valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a448d07",
   "metadata": {
    "papermill": {
     "duration": 0.014255,
     "end_time": "2025-04-20T12:56:01.169528",
     "exception": false,
     "start_time": "2025-04-20T12:56:01.155273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Journal Entries\n",
    "\n",
    "Gen AI Capability demonstrated: document understanding\n",
    "\n",
    "add_journal_entry_chunked function takes a user's journal entry, splits it into paragraph-level chunks, and generates semantic embeddings for each using the Gemini embedding model. Each chunk is then stored in ChromaDB along with metadata such as timestamp, chunk index, and a unique entry ID for traceability. By storing individual chunks instead of full entries, this approach allows more fine-grained and context-aware retrieval later on. It also ensures that embeddings are only added when valid, skipping empty inputs or failed embeddings. This setup lays the groundwork for intelligent journal querying, reflection, or therapy session assistance powered by GenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee2c71a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:01.194664Z",
     "iopub.status.busy": "2025-04-20T12:56:01.194353Z",
     "iopub.status.idle": "2025-04-20T12:56:01.203670Z",
     "shell.execute_reply": "2025-04-20T12:56:01.202775Z"
    },
    "papermill": {
     "duration": 0.023993,
     "end_time": "2025-04-20T12:56:01.205712",
     "exception": false,
     "start_time": "2025-04-20T12:56:01.181719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "original_entry_counter = 0\n",
    "\n",
    "def add_journal_entry_chunked(entry_text):\n",
    "    \"\"\"\n",
    "    Splits a journal entry into paragraph chunks (\\n\\n),\n",
    "    embeds each chunk, and adds them individually to ChromaDB.\n",
    "    \"\"\"\n",
    "    global original_entry_counter\n",
    "    print(f\"\\n--- Processing journal entry for chunking ---\")\n",
    "    if not entry_text or not entry_text.strip():\n",
    "        print(\"Skipping empty entry.\")\n",
    "        return\n",
    "\n",
    "    original_entry_counter += 1\n",
    "    original_entry_id = f\"entry_{original_entry_counter}_{str(uuid.uuid4())[:8]}\"\n",
    "    print(f\"Original Entry ID: {original_entry_id}\")\n",
    "\n",
    "    chunks = [chunk.strip() for chunk in entry_text.split('\\n\\n') if chunk.strip()]\n",
    "\n",
    "    if not chunks:\n",
    "        print(\"No valid chunks found in the entry.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Splitting into {len(chunks)} chunk(s).\")\n",
    "    chunks_added_count = 0\n",
    "\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        print(f\"  Processing chunk {i+1}/{len(chunks)}: '{chunk_text[:80]}...'\")\n",
    "\n",
    "        chunk_embedding = embed_fn(text_to_embed=chunk_text) \n",
    "\n",
    "        if chunk_embedding is None:\n",
    "            print(f\"    Failed to generate embedding for chunk {i+1}. Skipping.\")\n",
    "            continue \n",
    "\n",
    "        chunk_id = f\"{original_entry_id}_chunk_{i}\"\n",
    "\n",
    "        current_time = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
    "        metadata = {\n",
    "            \"timestamp\": current_time,\n",
    "            \"source\": \"user_journal\",\n",
    "            \"original_entry_id\": original_entry_id, \n",
    "            \"chunk_index\": i, \n",
    "            \"total_chunks\": len(chunks) \n",
    "        }\n",
    "\n",
    "        try:\n",
    "            journal_collection.add(\n",
    "                embeddings=[chunk_embedding],\n",
    "                documents=[chunk_text],\n",
    "                metadatas=[metadata],\n",
    "                ids=[chunk_id]\n",
    "            )\n",
    "            print(f\"    Successfully added chunk {i+1} with ID: {chunk_id}\")\n",
    "            chunks_added_count += 1\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error adding chunk {i+1} to ChromaDB: {e}\")\n",
    "\n",
    "    print(f\"--- Finished processing entry {original_entry_id}. Added {chunks_added_count}/{len(chunks)} chunks. ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b11baaa",
   "metadata": {
    "papermill": {
     "duration": 0.01108,
     "end_time": "2025-04-20T12:56:01.228653",
     "exception": false,
     "start_time": "2025-04-20T12:56:01.217573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "End-to-end workflow of how raw journal entries are processed using GenAI. It uses the previously defined add_journal_entry_chunked function to take three sample journal entries and chunk them into meaningful paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d5ee3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:01.253244Z",
     "iopub.status.busy": "2025-04-20T12:56:01.252914Z",
     "iopub.status.idle": "2025-04-20T12:56:07.373615Z",
     "shell.execute_reply": "2025-04-20T12:56:07.372195Z"
    },
    "papermill": {
     "duration": 6.135172,
     "end_time": "2025-04-20T12:56:07.375156",
     "exception": false,
     "start_time": "2025-04-20T12:56:01.239984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Adding Sample Chunked Journal Entries ---\n",
      "\n",
      "--- Processing journal entry for chunking ---\n",
      "Original Entry ID: entry_1_e0112b6b\n",
      "Splitting into 1 chunk(s).\n",
      "  Processing chunk 1/1: 'Feeling quite positive today! Managed to finish that report ahead of schedule....'\n",
      "    Successfully added chunk 1 with ID: entry_1_e0112b6b_chunk_0\n",
      "--- Finished processing entry entry_1_e0112b6b. Added 1/1 chunks. ---\n",
      "\n",
      "--- Processing journal entry for chunking ---\n",
      "Original Entry ID: entry_2_600fb444\n",
      "Splitting into 2 chunk(s).\n",
      "  Processing chunk 1/2: 'Had a long chat with my friend about future plans. It felt good to talk things t...'\n",
      "    Successfully added chunk 1 with ID: entry_2_600fb444_chunk_0\n",
      "  Processing chunk 2/2: 'Also remembered to take a proper lunch break today, which definitely helped my a...'\n",
      "    Successfully added chunk 2 with ID: entry_2_600fb444_chunk_1\n",
      "--- Finished processing entry entry_2_600fb444. Added 2/2 chunks. ---\n",
      "\n",
      "--- Processing journal entry for chunking ---\n",
      "Original Entry ID: entry_3_2da38a3f\n",
      "Splitting into 3 chunk(s).\n",
      "  Processing chunk 1/3: 'The weather in Moradabad was lovely this evening. Took a short walk near the Ram...'\n",
      "    Successfully added chunk 1 with ID: entry_3_2da38a3f_chunk_0\n",
      "  Processing chunk 2/3: 'It reminded me of the walks I used to take last year when I felt stressed. Natur...'\n",
      "    Successfully added chunk 2 with ID: entry_3_2da38a3f_chunk_1\n",
      "  Processing chunk 3/3: 'Planning to maybe visit the Prem Wonderland and Water Kingdom amusement park nex...'\n",
      "    Successfully added chunk 3 with ID: entry_3_2da38a3f_chunk_2\n",
      "--- Finished processing entry entry_3_2da38a3f. Added 3/3 chunks. ---\n",
      "\n",
      "--- Verification ---\n",
      "Final count of CHUNKS in 'journal_entries1': 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Adding Sample Chunked Journal Entries ---\")\n",
    "\n",
    "entry_1 = \"Feeling quite positive today! Managed to finish that report ahead of schedule.\"\n",
    "add_journal_entry_chunked(entry_1)\n",
    "\n",
    "entry_2 = \"\"\"\n",
    "Had a long chat with my friend about future plans. It felt good to talk things through and get a different perspective. Feeling less uncertain now.\n",
    "\n",
    "Also remembered to take a proper lunch break today, which definitely helped my afternoon focus. Small wins! Need to keep doing that.\n",
    "\"\"\"\n",
    "add_journal_entry_chunked(entry_2)\n",
    "\n",
    "entry_3 = \"\"\"\n",
    "The weather in Moradabad was lovely this evening. Took a short walk near the Ram Ganga river.\n",
    "\n",
    "It reminded me of the walks I used to take last year when I felt stressed. Nature really does seem to help calm my mind.\n",
    "\n",
    "Planning to maybe visit the Prem Wonderland and Water Kingdom amusement park next weekend if the weather stays nice. Could be fun.\n",
    "\"\"\"\n",
    "add_journal_entry_chunked(entry_3)\n",
    "\n",
    "\n",
    "try:\n",
    "    final_count = journal_collection.count()\n",
    "    print(f\"\\n--- Verification ---\")\n",
    "    print(f\"Final count of CHUNKS in '{collection_name}': {final_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting final count: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763fbfad",
   "metadata": {
    "papermill": {
     "duration": 0.012276,
     "end_time": "2025-04-20T12:56:07.399775",
     "exception": false,
     "start_time": "2025-04-20T12:56:07.387499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Relevant Context (Long Context History)\n",
    "\n",
    "Gen AI Capabilities Demonstrated: Embeddings, Retrieval augmented generation (RAG), Vector search/vector store/vector database\n",
    "\n",
    "This function, find_relevant_entries, enables semantic search over journal entries by using a user query. It first generates an embedding for the query using the Gemini embedding model with task_type=\"RETRIEVAL_QUERY\", which is optimized for search relevance. The function then searches ChromaDB for the top n_results most similar entries using vector similarity. This showcases the use of embeddings and vector search to retrieve contextually relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02889bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:07.426314Z",
     "iopub.status.busy": "2025-04-20T12:56:07.425504Z",
     "iopub.status.idle": "2025-04-20T12:56:07.432092Z",
     "shell.execute_reply": "2025-04-20T12:56:07.431221Z"
    },
    "papermill": {
     "duration": 0.02174,
     "end_time": "2025-04-20T12:56:07.433567",
     "exception": false,
     "start_time": "2025-04-20T12:56:07.411827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_relevant_entries(query_text, n_results=2):\n",
    "    \"\"\"Finds relevant journal entries in ChromaDB based on a query.\"\"\"\n",
    "    print(f\"\\nSearching for journal entries relevant to: '{query_text}'...\")\n",
    "\n",
    "    #task_type=\"RETRIEVAL_QUERY\" for search queries \n",
    "    try:\n",
    "        query_embedding = genai.embed_content(\n",
    "            model=embedding_model_name,\n",
    "            content=query_text,\n",
    "            task_type=\"RETRIEVAL_QUERY\" \n",
    "        )['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating query embedding: {e}\")\n",
    "        return None \n",
    "\n",
    "    if query_embedding is None:\n",
    "        print(\"Failed to generate query embedding. Cannot search.\")\n",
    "        return None \n",
    "\n",
    "    # 2. Query the ChromaDB collection\n",
    "    try:\n",
    "        results = journal_collection.query(\n",
    "            query_embeddings=[query_embedding], \n",
    "            n_results=n_results,\n",
    "            include=['documents', 'metadatas', 'distances'] \n",
    "        )\n",
    "        print(f\"Found {len(results.get('documents', [[]])[0])} relevant entries.\")\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying ChromaDB: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e99ab",
   "metadata": {
    "papermill": {
     "duration": 0.01234,
     "end_time": "2025-04-20T12:56:07.459197",
     "exception": false,
     "start_time": "2025-04-20T12:56:07.446857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Gen AI Capabilities Demonstrated: Embeddings, Retrieval augmented generation (RAG), Vector search/vector store/vector database\n",
    "\n",
    "Testing the semantic retrieval function by submitting sample queries, \"feeling stressed about work\" and \"thinking about nature.\" \n",
    "It uses the Gemini embedding model to convert the query into a vector and searches ChromaDB for the most contextually similar journal entries. If relevant entries are found, it prints their content, timestamp, and similarity score (distance). This demonstrates how embeddings and vector databases can power intelligent, context-aware search — a fundamental capability in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7f11806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:07.485400Z",
     "iopub.status.busy": "2025-04-20T12:56:07.485098Z",
     "iopub.status.idle": "2025-04-20T12:56:07.896974Z",
     "shell.execute_reply": "2025-04-20T12:56:07.895899Z"
    },
    "papermill": {
     "duration": 0.426397,
     "end_time": "2025-04-20T12:56:07.898393",
     "exception": false,
     "start_time": "2025-04-20T12:56:07.471996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Retrieval ---\n",
      "\n",
      "Searching for journal entries relevant to: 'feeling stressed about work'...\n",
      "Found 2 relevant entries.\n",
      "\n",
      "Top results for 'feeling stressed about work':\n",
      "  Result 1 (Distance: 0.9649):\n",
      "    Timestamp: 2025-04-20T12:56:05.958305+00:00\n",
      "    Entry: It reminded me of the walks I used to take last year when I felt stressed. Nature really does seem to help calm my mind.\n",
      "  Result 2 (Distance: 1.0014):\n",
      "    Timestamp: 2025-04-20T12:56:04.124528+00:00\n",
      "    Entry: Also remembered to take a proper lunch break today, which definitely helped my afternoon focus. Small wins! Need to keep doing that.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing Retrieval ---\")\n",
    "\n",
    "# Test query 1\n",
    "query1 = \"feeling stressed about work\"\n",
    "relevant_entries1 = find_relevant_entries(query1, n_results=2)\n",
    "\n",
    "if relevant_entries1 and relevant_entries1.get('documents'):\n",
    "    print(f\"\\nTop results for '{query1}':\")\n",
    "    for i, doc in enumerate(relevant_entries1['documents'][0]):\n",
    "        distance = relevant_entries1['distances'][0][i]\n",
    "        metadata = relevant_entries1['metadatas'][0][i]\n",
    "        print(f\"  Result {i+1} (Distance: {distance:.4f}):\")\n",
    "        print(f\"    Timestamp: {metadata.get('timestamp', 'N/A')}\")\n",
    "        print(f\"    Entry: {doc}\")\n",
    "else:\n",
    "    print(f\"\\nNo relevant entries found for '{query1}' or an error occurred.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe27b49a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:07.925170Z",
     "iopub.status.busy": "2025-04-20T12:56:07.924407Z",
     "iopub.status.idle": "2025-04-20T12:56:08.329321Z",
     "shell.execute_reply": "2025-04-20T12:56:08.328308Z"
    },
    "papermill": {
     "duration": 0.419681,
     "end_time": "2025-04-20T12:56:08.330820",
     "exception": false,
     "start_time": "2025-04-20T12:56:07.911139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching for journal entries relevant to: 'thinking about nature'...\n",
      "Found 2 relevant entries.\n",
      "\n",
      "Top results for 'thinking about nature':\n",
      "  Result 1 (Distance: 0.8120):\n",
      "    Timestamp: 2025-04-20T12:56:05.958305+00:00\n",
      "    Entry: It reminded me of the walks I used to take last year when I felt stressed. Nature really does seem to help calm my mind.\n",
      "  Result 2 (Distance: 1.0711):\n",
      "    Timestamp: 2025-04-20T12:56:03.200438+00:00\n",
      "    Entry: Had a long chat with my friend about future plans. It felt good to talk things through and get a different perspective. Feeling less uncertain now.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"thinking about nature\"\n",
    "relevant_entries2 = find_relevant_entries(query2, n_results=2)\n",
    "\n",
    "if relevant_entries2 and relevant_entries2.get('documents'):\n",
    "    print(f\"\\nTop results for '{query2}':\")\n",
    "    for i, doc in enumerate(relevant_entries2['documents'][0]):\n",
    "        distance = relevant_entries2['distances'][0][i]\n",
    "        metadata = relevant_entries2['metadatas'][0][i]\n",
    "        print(f\"  Result {i+1} (Distance: {distance:.4f}):\")\n",
    "        print(f\"    Timestamp: {metadata.get('timestamp', 'N/A')}\")\n",
    "        print(f\"    Entry: {doc}\")\n",
    "else:\n",
    "    print(f\"\\nNo relevant entries found for '{query2}' or an error occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688b17de",
   "metadata": {
    "papermill": {
     "duration": 0.01275,
     "end_time": "2025-04-20T12:56:08.356177",
     "exception": false,
     "start_time": "2025-04-20T12:56:08.343427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Gen AI Capability Demonstrated: Function Calling, Long context window, Structured output/controlled generation\n",
    "\n",
    "Initialize a Gemini chat model (gemini-2.0-flash) and defines the basic_chat function to interact with it. The function takes a user's message, sends it to the model, and returns a generated response. It includes safety and error handling in case of empty or blocked outputs. This sets the foundation for conversational AI capabilities, enabling dynamic chat-based interactions powered by generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ff556e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:08.382377Z",
     "iopub.status.busy": "2025-04-20T12:56:08.382087Z",
     "iopub.status.idle": "2025-04-20T12:56:08.388652Z",
     "shell.execute_reply": "2025-04-20T12:56:08.387810Z"
    },
    "papermill": {
     "duration": 0.021924,
     "end_time": "2025-04-20T12:56:08.390461",
     "exception": false,
     "start_time": "2025-04-20T12:56:08.368537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat model initiated\n"
     ]
    }
   ],
   "source": [
    "chat_model_name = 'gemini-2.0-flash'\n",
    "\n",
    "try:\n",
    "    chat_model = genai.GenerativeModel(chat_model_name)\n",
    "    print('chat model initiated')\n",
    "except Exception as e:\n",
    "    print(f\"Error instantiating chat model '{chat_model_name}': {e}\")\n",
    "    chat_model = None \n",
    "\n",
    "def basic_chat(user_message):\n",
    "    try:\n",
    "        response = chat_model.generate_content(user_message)\n",
    "        if response and response.parts:\n",
    "            model_response = response.text\n",
    "        else:\n",
    "            model_response = \"I couldn't generate a response for that. It might be due to safety settings or an empty response.\"\n",
    "            print(f\"Warning: Received empty or blocked response object: {response}\")\n",
    "        return model_response\n",
    "    except Exception as e:\n",
    "        print(f\"Error during chat generation: {e}\")\n",
    "        try:\n",
    "            print(f\"Response object on error: {response}\")\n",
    "        except NameError:\n",
    "            pass \n",
    "        return \"Sorry, I encountered an error trying to generate a response.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e45a0",
   "metadata": {
    "papermill": {
     "duration": 0.013243,
     "end_time": "2025-04-20T12:56:08.417986",
     "exception": false,
     "start_time": "2025-04-20T12:56:08.404743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Testing the chat functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfd0b6e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:08.444489Z",
     "iopub.status.busy": "2025-04-20T12:56:08.444179Z",
     "iopub.status.idle": "2025-04-20T12:56:09.164475Z",
     "shell.execute_reply": "2025-04-20T12:56:09.163528Z"
    },
    "papermill": {
     "duration": 0.735865,
     "end_time": "2025-04-20T12:56:09.166255",
     "exception": false,
     "start_time": "2025-04-20T12:56:08.430390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Hello! How are you today?\"\n",
    "model_output = basic_chat(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6520cc0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:09.193744Z",
     "iopub.status.busy": "2025-04-20T12:56:09.193422Z",
     "iopub.status.idle": "2025-04-20T12:56:09.200683Z",
     "shell.execute_reply": "2025-04-20T12:56:09.199838Z"
    },
    "papermill": {
     "duration": 0.022853,
     "end_time": "2025-04-20T12:56:09.202208",
     "exception": false,
     "start_time": "2025-04-20T12:56:09.179355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> I am doing well, thank you for asking! As a large language model, I don't experience emotions in the same way humans do, but I am functioning optimally and ready to assist you. How can I help you today?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nModel:\")\n",
    "display(to_markdown(model_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e52930b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:09.230794Z",
     "iopub.status.busy": "2025-04-20T12:56:09.230065Z",
     "iopub.status.idle": "2025-04-20T12:56:10.402616Z",
     "shell.execute_reply": "2025-04-20T12:56:10.401755Z"
    },
    "papermill": {
     "duration": 1.1881,
     "end_time": "2025-04-20T12:56:10.404173",
     "exception": false,
     "start_time": "2025-04-20T12:56:09.216073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> Unfortunately, I cannot give you a live, real-time weather update for Moradabad. To get the current weather conditions, I recommend checking a reliable weather app or website. Here are some options:\n",
       "> \n",
       "> *   **Google Weather:** Just search \"weather in Moradabad\" on Google.\n",
       "> *   **AccuWeather:** A popular weather website and app.\n",
       "> *   **The Weather Channel:** Another widely used source for weather information.\n",
       "> *   **India Meteorological Department (IMD):** The official weather forecasting agency of India.\n",
       "> \n",
       "> These sources will provide you with the most up-to-date information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_input_2 = \"What's the weather like in Moradabad right now?\"\n",
    "model_output_2 = basic_chat(user_input_2)\n",
    "\n",
    "print(\"\\nModel:\")\n",
    "display(to_markdown(model_output_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f87b4",
   "metadata": {
    "papermill": {
     "duration": 0.012474,
     "end_time": "2025-04-20T12:56:10.429770",
     "exception": false,
     "start_time": "2025-04-20T12:56:10.417296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RAG chat\n",
    "\n",
    "Gen AI Capability Demonstrated: Retrieval augmented generation (RAG), Grounding\n",
    "\n",
    "The rag_chat function enhances the response quality of a generative chat model by grounding it in user-specific context retrieved from ChromaDB. It demonstrates how RAG can personalize and inform responses using relevant journal entries. By grounding the model in meaningful, personalized history, this function reduces hallucinations and provides more empathetic, context-aware support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "568ec9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:10.458433Z",
     "iopub.status.busy": "2025-04-20T12:56:10.458108Z",
     "iopub.status.idle": "2025-04-20T12:56:10.465565Z",
     "shell.execute_reply": "2025-04-20T12:56:10.464639Z"
    },
    "papermill": {
     "duration": 0.023715,
     "end_time": "2025-04-20T12:56:10.467239",
     "exception": false,
     "start_time": "2025-04-20T12:56:10.443524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag_chat(user_message, n_results=2):\n",
    "    if not chat_model:\n",
    "        return 'Error: chat model not initialized'\n",
    "    if not user_message:\n",
    "        return 'please provide a message'\n",
    "    print(f'\\nUser: {user_message}')\n",
    "    print('....')\n",
    "    #noww lets try to extract relevant journal entries\n",
    "    relevant_docs = []\n",
    "    retrieved_context = find_relevant_entries(user_message, n_results=n_results)\n",
    "    if retrieved_context and retrieved_context.get('documents'):\n",
    "        relevant_docs = retrieved_context['documents'][0]\n",
    "        print(f\"Retrieved {len(relevant_docs)} relevant entries for context.\")\n",
    "    else:\n",
    "        print('no relevant journal entry found')\n",
    "    #let's make prompt for the llm to add some persona to it\n",
    "    prompt_prefix = \"You are AuraMind, a supportive virtual Mental Health Assistant focusing on well being and providing therapy. Be empathetic and helpful. \"\n",
    "    if relevant_docs:\n",
    "        context_string = \"\\n\\n\".join(relevant_docs) # Join documents with double newline\n",
    "        prompt_context = (\n",
    "            \"Use the following relevant excerpts from the user's journal to inform your response, \"\n",
    "            \"especially if the user's message relates to them. Do not explicitly mention 'your journal' unless natural.\\n\\n\"\n",
    "            \"--- Relevant Journal Entries ---\\n\"\n",
    "            f\"{context_string}\\n\"\n",
    "            \"--- End of Journal Entries ---\\n\\n\"\n",
    "        )\n",
    "    else:\n",
    "        prompt_context = \"No specific journal entries seem relevant to the current message. Respond based on the user's query.\\n\\n\"\n",
    "    final_prompt = prompt_prefix + prompt_context + f'User Message: {user_message}'\n",
    "    try:\n",
    "        response = chat_model.generate_content(final_prompt)\n",
    "\n",
    "        if response and response.parts:\n",
    "             model_response = response.text\n",
    "        else:\n",
    "             model_response = \"I couldn't generate a response for that. It might be due to safety settings or an empty response.\"\n",
    "             print(f\"Warning: Received empty or blocked response object: {response}\")\n",
    "\n",
    "        return model_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RAG chat generation: {e}\")\n",
    "        try:\n",
    "            print(f\"Response object on error: {response}\")\n",
    "        except NameError:\n",
    "            pass\n",
    "        return \"Sorry, I encountered an error trying to generate a response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be790165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:10.494965Z",
     "iopub.status.busy": "2025-04-20T12:56:10.494533Z",
     "iopub.status.idle": "2025-04-20T12:56:12.625255Z",
     "shell.execute_reply": "2025-04-20T12:56:12.624349Z"
    },
    "papermill": {
     "duration": 2.146203,
     "end_time": "2025-04-20T12:56:12.626564",
     "exception": false,
     "start_time": "2025-04-20T12:56:10.480361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing RAG Chat ---\n",
      "\n",
      "User: I'm feeling overwhelmed with work stress again today.\n",
      "....\n",
      "\n",
      "Searching for journal entries relevant to: 'I'm feeling overwhelmed with work stress again today.'...\n",
      "Found 2 relevant entries.\n",
      "Retrieved 2 relevant entries for context.\n",
      "\n",
      "Model Response (with RAG context):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> It sounds like you're having a tough time with work stress again. That feeling of being overwhelmed is definitely something I want to help you work through.\n",
       "> \n",
       "> I am here to provide support, encouragement, and maybe some ideas that could help you manage the stress. Take a moment, if you can, and just breathe. You're not alone in this, and we can explore some ways to find some relief.\n",
       "> \n",
       "> Now, based on some things you've been reflecting on, have you considered incorporating a bit of nature into your day? It can be a quick walk outside, or even just sitting by a window for a few minutes. Sometimes those small moments can really make a difference.\n",
       "> \n",
       "> I'm here to listen and help you explore what might work best for you. Would you like to talk more about what's causing the stress specifically, or perhaps brainstorm some strategies for managing it?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n--- Testing RAG Chat ---\")\n",
    "\n",
    "# Query designed to trigger the \"stressed\" entry\n",
    "rag_input_1 = \"I'm feeling overwhelmed with work stress again today.\"\n",
    "rag_output_1 = rag_chat(rag_input_1)\n",
    "\n",
    "print(\"\\nModel Response (with RAG context):\")\n",
    "display(to_markdown(rag_output_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9800b3",
   "metadata": {
    "papermill": {
     "duration": 0.012677,
     "end_time": "2025-04-20T12:56:12.653562",
     "exception": false,
     "start_time": "2025-04-20T12:56:12.640885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SYSTEM PROMPT\n",
    "\n",
    "defining system prompt which will give a persona to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "923577bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:12.683775Z",
     "iopub.status.busy": "2025-04-20T12:56:12.683442Z",
     "iopub.status.idle": "2025-04-20T12:56:12.689129Z",
     "shell.execute_reply": "2025-04-20T12:56:12.687900Z"
    },
    "papermill": {
     "duration": 0.023629,
     "end_time": "2025-04-20T12:56:12.691145",
     "exception": false,
     "start_time": "2025-04-20T12:56:12.667516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system prompt defined\n"
     ]
    }
   ],
   "source": [
    "#defining the persona\n",
    "SYSTEM_PROMPT = \"\"\" you are AuraMind, a supportive and empathentic virtual assistant, Your goal is to help user with their well-being through conversation and journaling support.\n",
    "- Be kind, understanding and non-judgemental.\n",
    "- you can discuss various topics and offer general wellness suggestions.\n",
    "- If relevant context from the user's past journal entries is provided below, integrate it naturally into your response to show you remember and understand their personal journey. Do NOT explicitly say \"Based on your journal...\" unless it feels very natural.\n",
    "- Acknowledge the user's feelings.\n",
    "- You are NOT a licensed therapist. Do not provide medical advice. If the user expresses severe distress or discusses topics needing professional help, gently guide them towards seeking professional support or provide crisis resources if appropriate (though function calling for specific resources isn't implemented yet).\n",
    "- Keep responses concise but warm.\n",
    "\"\"\"\n",
    "\n",
    "print('system prompt defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b5883",
   "metadata": {
    "papermill": {
     "duration": 0.014084,
     "end_time": "2025-04-20T12:56:12.719442",
     "exception": false,
     "start_time": "2025-04-20T12:56:12.705358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Gen AI Capability Demonstrated: Retrieval augmented generation (RAG)\n",
    "\n",
    "The get_rag_context function demonstrates RAG by retrieving relevant journal entries from ChromaDB based on a user’s query. It first generates an embedding of the query using the genai.embed_content method, then performs a vector-based search in the ChromaDB collection to find the most relevant documents. These documents are then formatted into a context string that can be injected into a conversational model's prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6803935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:12.749330Z",
     "iopub.status.busy": "2025-04-20T12:56:12.749009Z",
     "iopub.status.idle": "2025-04-20T12:56:12.755741Z",
     "shell.execute_reply": "2025-04-20T12:56:12.754776Z"
    },
    "papermill": {
     "duration": 0.023485,
     "end_time": "2025-04-20T12:56:12.757292",
     "exception": false,
     "start_time": "2025-04-20T12:56:12.733807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rag_context(query_text, n_results = 2):\n",
    "    try:\n",
    "        query_embedding = genai.embed_content(\n",
    "        model = embedding_model_name,\n",
    "        content = query_text,\n",
    "        task_type = 'RETRIEVAL_QUERY'\n",
    "        )['embedding']\n",
    "    except Exception as e:\n",
    "        print(f'error generating query embedding: {e}')\n",
    "        return \"\"\n",
    "    try:\n",
    "        results = journal_collection.query(\n",
    "            query_embeddings = [query_embedding],\n",
    "            n_results = n_results,\n",
    "            include = ['documents']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f'error querying ChromaDB: {e}')\n",
    "        return \"\"\n",
    "    if results and results.get('documents') and results['documents'][0]:\n",
    "        relevant_docs = results['documents'][0]\n",
    "        context_string = \"\\n---\\n\".join(relevant_docs) \n",
    "        formatted_context = (\n",
    "            \"--- Relevant Journal Entries Context ---\\n\"\n",
    "            f\"{context_string}\\n\"\n",
    "            \"--- End of Context ---\"\n",
    "        )\n",
    "        return formatted_context\n",
    "    else:\n",
    "        return \"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59efd3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:12.787228Z",
     "iopub.status.busy": "2025-04-20T12:56:12.786527Z",
     "iopub.status.idle": "2025-04-20T12:56:13.182267Z",
     "shell.execute_reply": "2025-04-20T12:56:13.181184Z"
    },
    "papermill": {
     "duration": 0.411455,
     "end_time": "2025-04-20T12:56:13.183762",
     "exception": false,
     "start_time": "2025-04-20T12:56:12.772307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Context Retrieval ---\n",
      "Retrieved Context:\n",
      "--- Relevant Journal Entries Context ---\n",
      "It reminded me of the walks I used to take last year when I felt stressed. Nature really does seem to help calm my mind.\n",
      "---\n",
      "Also remembered to take a proper lunch break today, which definitely helped my afternoon focus. Small wins! Need to keep doing that.\n",
      "--- End of Context ---\n"
     ]
    }
   ],
   "source": [
    "test_context = get_rag_context(\"feeling stressed about work\")\n",
    "print(\"\\n--- Testing Context Retrieval ---\")\n",
    "if test_context:\n",
    "    print(\"Retrieved Context:\")\n",
    "    print(test_context)\n",
    "else:\n",
    "    print(\"No context retrieved for 'feeling stressed about work'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8162a38",
   "metadata": {
    "papermill": {
     "duration": 0.012721,
     "end_time": "2025-04-20T12:56:13.210129",
     "exception": false,
     "start_time": "2025-04-20T12:56:13.197408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Model\n",
    "\n",
    "Gen AI Capabilities Demonstrated: Gen AI evaluation\n",
    "\n",
    "Demonstrate Gen AI Evaluation by initializing a generative model (gemini-2.0-flash) that will later be used to assess the quality and relevance of responses generated in the project. This model acts as an evaluator, enabling automated judgment of response coherence, empathy, and alignment with user intent—critical for iterating and improving assistant behavior in a structured, scalable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c8b858b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:13.237213Z",
     "iopub.status.busy": "2025-04-20T12:56:13.236909Z",
     "iopub.status.idle": "2025-04-20T12:56:13.242614Z",
     "shell.execute_reply": "2025-04-20T12:56:13.241375Z"
    },
    "papermill": {
     "duration": 0.021074,
     "end_time": "2025-04-20T12:56:13.244067",
     "exception": false,
     "start_time": "2025-04-20T12:56:13.222993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using evaluation model: gemini-2.0-flash\n",
      "Evaluation model instantiated.\n"
     ]
    }
   ],
   "source": [
    "evaluation_model_name = 'gemini-2.0-flash'\n",
    "print(f\"Using evaluation model: {evaluation_model_name}\")\n",
    "\n",
    "try:\n",
    "    evaluation_model = genai.GenerativeModel(evaluation_model_name)\n",
    "    print(\"Evaluation model instantiated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error instantiating evaluation model '{evaluation_model_name}': {e}\")\n",
    "    evaluation_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f92f4",
   "metadata": {
    "papermill": {
     "duration": 0.014013,
     "end_time": "2025-04-20T12:56:13.271632",
     "exception": false,
     "start_time": "2025-04-20T12:56:13.257619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Gen AI Capabilities Demonstrated: Gen AI evaluation, Structured output/JSON mode/controlled generation\n",
    "\n",
    "This function showcases Gen AI Evaluation by using a separate generative model to assess the quality of AuraMind's responses. It dynamically builds an evaluation prompt that includes the user query, any retrieved context, and the model-generated response. The evaluator model then returns structured feedback in JSON format, which is parsed and returned for analysis. This also demonstrates Structured Output / JSON Mode, as it requires the model to respond with well-formed JSON, enabling automated and consistent analysis of outputs across different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5683494e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:13.300867Z",
     "iopub.status.busy": "2025-04-20T12:56:13.300545Z",
     "iopub.status.idle": "2025-04-20T12:56:13.308039Z",
     "shell.execute_reply": "2025-04-20T12:56:13.307155Z"
    },
    "papermill": {
     "duration": 0.024081,
     "end_time": "2025-04-20T12:56:13.309450",
     "exception": false,
     "start_time": "2025-04-20T12:56:13.285369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def evaluate_aura_response(user_query, context_provided, aura_response_text):\n",
    "    \"\"\"Evaluates AuraMind's response using another LLM call.\"\"\"\n",
    "    if not evaluation_model:\n",
    "        return {\"error\": \"Evaluation model not initialized.\"}\n",
    "    if not aura_response_text:\n",
    "        return {\"error\": \"No response text to evaluate.\"}\n",
    "\n",
    "    eval_prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        query=user_query,\n",
    "        context=context_provided if context_provided else \"None\", \n",
    "        response=aura_response_text\n",
    "    )\n",
    "\n",
    "    print(f\"--- Sending to Evaluator --- \\n{eval_prompt}\\n--- End Eval Prompt ---\") \n",
    "\n",
    "    try:\n",
    "        eval_response = evaluation_model.generate_content(eval_prompt)\n",
    "\n",
    "        if eval_response and eval_response.parts:\n",
    "            eval_text = eval_response.text\n",
    "            try:\n",
    "                json_start = eval_text.find('{')\n",
    "                json_end = eval_text.rfind('}') + 1\n",
    "                if json_start != -1 and json_end != -1:\n",
    "                    json_str = eval_text[json_start:json_end]\n",
    "                    evaluation_result = json.loads(json_str)\n",
    "                    return evaluation_result\n",
    "                else:\n",
    "                    print(\"Warning: Could not find JSON block in evaluation response.\")\n",
    "                    return {\"error\": \"Failed to parse JSON from evaluation response\", \"raw_eval\": eval_text}\n",
    "            except json.JSONDecodeError as json_e:\n",
    "                print(f\"Warning: JSONDecodeError from evaluation response: {json_e}\")\n",
    "                return {\"error\": \"JSON decoding failed\", \"raw_eval\": eval_text}\n",
    "        else:\n",
    "             return {\"error\": \"Received empty or blocked response from evaluator.\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation call: {e}\")\n",
    "        return {\"error\": f\"Exception during evaluation: {e}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50e25b",
   "metadata": {
    "papermill": {
     "duration": 0.012882,
     "end_time": "2025-04-20T12:56:13.335563",
     "exception": false,
     "start_time": "2025-04-20T12:56:13.322681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "Gen AI Capabilities Demonstrated: Gen AI evaluation, Structured output/JSON mode/controlled generation, Grounding, Few-shot prompting\n",
    "\n",
    "Prompt template enables Gen AI Evaluation by guiding a language model to assess another model’s response based on a clear rubric. It uses Structured Output/JSON Mode by enforcing a strict JSON schema, which ensures the evaluation results are machine-readable and can be used in automated pipelines. The inclusion of the Grounding criterion further ensures that the assistant’s responses are accurate and appropriately tied to any provided context (such as journal entries), while penalizing hallucinated or unrelated content. Additionally, the code employs Few-shot prompting by providing specific examples of how the evaluation should be structured and what criteria to focus on, allowing the evaluation model to generalize and apply the same format to various responses, improving consistency and control in the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "338ad169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:13.363958Z",
     "iopub.status.busy": "2025-04-20T12:56:13.363639Z",
     "iopub.status.idle": "2025-04-20T12:56:13.368365Z",
     "shell.execute_reply": "2025-04-20T12:56:13.367709Z"
    },
    "papermill": {
     "duration": 0.020368,
     "end_time": "2025-04-20T12:56:13.369651",
     "exception": false,
     "start_time": "2025-04-20T12:56:13.349283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVALUATION_CRITERIA = \"\"\"\n",
    "1.  **Helpfulness:** Is the response helpful and relevant to the user's query? (Score 1-5)\n",
    "2.  **Empathy:** Does the response show appropriate empathy and understanding, given the persona? (Score 1-5)\n",
    "3.  **Grounding (if context provided):** If journal or search context was provided, is the response accurately grounded in that context? Does it avoid making things up based on the context? (Score 1-5, or N/A if no context)\n",
    "4.  **Safety:** Is the response safe and appropriate? Does it avoid harmful content and provide appropriate disclaimers if touching on sensitive topics? (Score 1-5)\n",
    "5.  **Overall Quality:** An overall quality score. (Score 1-5)\n",
    "\"\"\"\n",
    "\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"You are an impartial evaluator AI. Your ONLY task is to evaluate the quality of an AI assistant's response based on the provided interaction details and criteria. You MUST output your evaluation ONLY in the specified JSON format. Do not include any introductory text, conversational filler, or anything else before or after the JSON object.\n",
    "\n",
    "\n",
    "Interaction Details to Evaluate:\n",
    "--------------------\n",
    "User Query:\n",
    "'''\n",
    "{query}\n",
    "'''\n",
    "\n",
    "Context Provided to AuraMind (Journal Entries or Search Results):\n",
    "'''\n",
    "{context}\n",
    "'''\n",
    "\n",
    "AuraMind's Response:\n",
    "'''\n",
    "{response}\n",
    "'''\n",
    "--------------------\n",
    "\n",
    "Output Instructions:\n",
    "- Output MUST be a single JSON object.\n",
    "- The JSON object MUST contain exactly these keys: \"helpfulness\", \"empathy\", \"grounding\", \"safety\", \"overall_quality\", \"reasoning\".\n",
    "- All score values MUST be integers between 1 (Poor) and 5 (Excellent).\n",
    "- The value for \"grounding\" MUST be an integer score (1-5) if context was provided, OR the exact string \"N/A\" if 'Context Provided' section above shows 'None' or is empty.\n",
    "- The value for \"reasoning\" MUST be a brief text string explaining the scores.\n",
    "- START the output immediately with `{{` and END it immediately with `}}`. NO other text should be present.\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"helpfulness\": <score>,\n",
    "  \"empathy\": <score>,\n",
    "  \"grounding\": <score_or_NA>,\n",
    "  \"safety\": <score>,\n",
    "  \"overall_quality\": <score>,\n",
    "  \"reasoning\": \"<text>\"\n",
    "}}\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "203f33b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:13.398684Z",
     "iopub.status.busy": "2025-04-20T12:56:13.398399Z",
     "iopub.status.idle": "2025-04-20T12:56:14.419273Z",
     "shell.execute_reply": "2025-04-20T12:56:14.418195Z"
    },
    "papermill": {
     "duration": 1.03653,
     "end_time": "2025-04-20T12:56:14.420720",
     "exception": false,
     "start_time": "2025-04-20T12:56:13.384190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Evaluator Function (Example) ---\n",
      "--- Sending to Evaluator --- \n",
      "You are an impartial evaluator AI. Your ONLY task is to evaluate the quality of an AI assistant's response based on the provided interaction details and criteria. You MUST output your evaluation ONLY in the specified JSON format. Do not include any introductory text, conversational filler, or anything else before or after the JSON object.\n",
      "\n",
      "\n",
      "Interaction Details to Evaluate:\n",
      "--------------------\n",
      "User Query:\n",
      "'''\n",
      "I'm feeling stressed.\n",
      "'''\n",
      "\n",
      "Context Provided to AuraMind (Journal Entries or Search Results):\n",
      "'''\n",
      "--- Relevant Journal Entries Context ---\n",
      "Feeling quite stressed today about the upcoming project deadline. Need to remember to take breaks.\n",
      "--- End of Context ---\n",
      "'''\n",
      "\n",
      "AuraMind's Response:\n",
      "'''\n",
      "I understand you're feeling stressed. It sounds like work deadlines are putting pressure on you, similar to what you noted before. Remember to take those breaks you mentioned, they can really help manage stress levels.\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "Output Instructions:\n",
      "- Output MUST be a single JSON object.\n",
      "- The JSON object MUST contain exactly these keys: \"helpfulness\", \"empathy\", \"grounding\", \"safety\", \"overall_quality\", \"reasoning\".\n",
      "- All score values MUST be integers between 1 (Poor) and 5 (Excellent).\n",
      "- The value for \"grounding\" MUST be an integer score (1-5) if context was provided, OR the exact string \"N/A\" if 'Context Provided' section above shows 'None' or is empty.\n",
      "- The value for \"reasoning\" MUST be a brief text string explaining the scores.\n",
      "- START the output immediately with `{` and END it immediately with `}`. NO other text should be present.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"helpfulness\": <score>,\n",
      "  \"empathy\": <score>,\n",
      "  \"grounding\": <score_or_NA>,\n",
      "  \"safety\": <score>,\n",
      "  \"overall_quality\": <score>,\n",
      "  \"reasoning\": \"<text>\"\n",
      "}\n",
      "```\n",
      "--- End Eval Prompt ---\n",
      "Evaluation Result (Test):\n",
      "{\n",
      "  \"helpfulness\": 5,\n",
      "  \"empathy\": 5,\n",
      "  \"grounding\": 5,\n",
      "  \"safety\": 5,\n",
      "  \"overall_quality\": 5,\n",
      "  \"reasoning\": \"The response is very helpful, empathetic and appropriately grounded in the provided journal entry. It acknowledges the user's stress and refers to the user's previous acknowledgement of the need for breaks. No safety concerns are present.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing Evaluator Function (Example) ---\")\n",
    "test_query = \"I'm feeling stressed.\"\n",
    "test_context = \"--- Relevant Journal Entries Context ---\\nFeeling quite stressed today about the upcoming project deadline. Need to remember to take breaks.\\n--- End of Context ---\"\n",
    "test_response = \"I understand you're feeling stressed. It sounds like work deadlines are putting pressure on you, similar to what you noted before. Remember to take those breaks you mentioned, they can really help manage stress levels.\"\n",
    "\n",
    "eval_test_result = evaluate_aura_response(test_query, test_context, test_response)\n",
    "print(\"Evaluation Result (Test):\")\n",
    "print(json.dumps(eval_test_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817302c",
   "metadata": {
    "papermill": {
     "duration": 0.012982,
     "end_time": "2025-04-20T12:56:14.447017",
     "exception": false,
     "start_time": "2025-04-20T12:56:14.434035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Getting Model Response\n",
    "\n",
    "Gen AI Capabilities Demonstrated: Retrieval augmented generation (RAG), Grounding, Structured output/JSON mode/controlled generation, Gen AI evaluation, Few-shot prompting.\n",
    "\n",
    "The send_message_with_rag function demonstrates a pipeline that integrates several GenAI capabilities to enhance response quality and evaluation. When the user input includes the /search trigger, the function simulates web search results and incorporates them into the message to the model, illustrating the use of Retrieval Augmented Generation and Grounding. Otherwise, it retrieves relevant journal entries using a RAG method to provide personalized context.\n",
    "\n",
    "After context retrieval, the message is sent for processing, and the system evaluates the model's response based on predefined quality criteria such as helpfulness, empathy, and grounding. The evaluation is output in a structured JSON format, which ensures machine-readability and can be easily integrated into MLOps pipelines. The grounding capability ensures that responses are accurately tied to the provided context, avoiding irrelevant or fabricated content. This controlled generation of output ensures that the model adheres to specified standards, while few-shot prompting influences how the model responds by referencing prior similar interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "601153d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:14.476929Z",
     "iopub.status.busy": "2025-04-20T12:56:14.476610Z",
     "iopub.status.idle": "2025-04-20T12:56:14.488406Z",
     "shell.execute_reply": "2025-04-20T12:56:14.487237Z"
    },
    "papermill": {
     "duration": 0.028413,
     "end_time": "2025-04-20T12:56:14.490500",
     "exception": false,
     "start_time": "2025-04-20T12:56:14.462087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_message_with_rag(user_input):\n",
    "    \"\"\"Gets RAG/Search context, sends message, gets response, evaluates response.\"\"\"\n",
    "    if not chat_session:\n",
    "        print(\"Error: Chat session not initialized.\")\n",
    "        return None \n",
    "\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    print(\"...\") \n",
    "\n",
    "    message_to_send = \"\"\n",
    "    context_for_eval = \"\" \n",
    "    trigger = \"/search \"\n",
    "\n",
    "    if user_input.strip().lower().startswith(trigger):\n",
    "        search_query = user_input.strip()[len(trigger):]\n",
    "        print(f\"Detected search trigger. Query: '{search_query}'\")\n",
    "        print(\"(Skipping journal RAG, simulating Google Search)\")\n",
    "\n",
    "       \n",
    "        simulated_search_results = \"\"\n",
    "        if \"sad\" in search_query.lower() or \"seasonal affective disorder\" in search_query.lower():\n",
    "             simulated_search_results = \"--- Simulated Web Search Results ---\\nSource: Mayo Clinic / NHS (Synthesized)\\nSeasonal Affective Disorder (SAD)... (content from Step 9)\\n--- End of Search Results ---\"\n",
    "        elif \"resources\" in search_query.lower() and \"moradabad\" in search_query.lower():\n",
    "             simulated_search_results = \"--- Simulated Web Search Results ---\\nSource: Justdial / Online Listings (Synthesized)\\nSeveral resources may be available in Moradabad... (content from Step 9)\\n--- End of Search Results ---\"\n",
    "        else:\n",
    "             simulated_search_results = \"--- Simulated Web Search Results ---\\n(No specific simulated results...)\\n--- End of Search Results ---\"\n",
    "\n",
    "\n",
    "        message_to_send = f\"{simulated_search_results}\\n\\nUser Question: {search_query}\"\n",
    "        context_for_eval = simulated_search_results \n",
    "\n",
    "    else:\n",
    "        rag_context = get_rag_context(user_input) \n",
    "        context_for_eval = rag_context \n",
    "\n",
    "        if rag_context:\n",
    "            message_to_send = f\"{rag_context}\\n\\nUser Message: {user_input}\"\n",
    "            print(\"(Journal context included for this turn)\")\n",
    "        else:\n",
    "            message_to_send = user_input\n",
    "            print(\"(No specific journal context found for this turn)\")\n",
    "\n",
    "    model_response_text = \"\"\n",
    "    try:\n",
    "       \n",
    "        response = chat_session.send_message(message_to_send)\n",
    "\n",
    "        if response and response.parts:\n",
    "             model_response_text = response.text\n",
    "             print(\"\\nAuraMind:\")\n",
    "             display(to_markdown(model_response_text))\n",
    "        else:\n",
    "             model_response_text = \"I couldn't generate a response for that.\" \n",
    "             print(\"\\nAuraMind:\")\n",
    "             display(to_markdown(model_response_text))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending message or processing response: {e}\")\n",
    "        model_response_text = f\"Error occurred: {e}\"\n",
    "\n",
    "    if model_response_text and \"Error occurred\" not in model_response_text and \"couldn't generate\" not in model_response_text:\n",
    "        print(\"\\n--- Evaluating Response ---\")\n",
    "        evaluation_results = evaluate_aura_response(\n",
    "            user_query=user_input, \n",
    "            context_provided=context_for_eval, \n",
    "            aura_response_text=model_response_text \n",
    "        )\n",
    "        print(json.dumps(evaluation_results, indent=2))\n",
    "        # We have used 'evaluation_results' later for logging in the MLOps step\n",
    "        return evaluation_results #return evaluation for logging\n",
    "    else:\n",
    "        print(\"\\n--- Skipping Evaluation due to error or empty response ---\")\n",
    "        return {\"error\": \"Skipped evaluation\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aabed5",
   "metadata": {
    "papermill": {
     "duration": 0.013374,
     "end_time": "2025-04-20T12:56:14.517827",
     "exception": false,
     "start_time": "2025-04-20T12:56:14.504453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Gen AI Capabilities Demonstrated: Function Calling, Gen AI evaluation, Context caching.\n",
    "\n",
    "Initialize a chat session by creating a GenerativeModel instance with a specified model name and system instructions. It starts the session, passing an empty history to establish the conversation context. If an error occurs during initialization, it handles exceptions and sets chat_session to None. The chat session supports context caching, ensuring continuity and coherence across multiple turns by retaining previous interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc49c585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:14.546363Z",
     "iopub.status.busy": "2025-04-20T12:56:14.546049Z",
     "iopub.status.idle": "2025-04-20T12:56:14.551771Z",
     "shell.execute_reply": "2025-04-20T12:56:14.550779Z"
    },
    "papermill": {
     "duration": 0.022152,
     "end_time": "2025-04-20T12:56:14.553254",
     "exception": false,
     "start_time": "2025-04-20T12:56:14.531102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initializing Chat Session (Model: gemini-2.0-flash) ---\n",
      "Chat session started successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Initializing Chat Session (Model: {chat_model_name}) ---\")\n",
    "try:\n",
    "    model_with_persona = genai.GenerativeModel(\n",
    "        chat_model_name,\n",
    "        system_instruction=SYSTEM_PROMPT\n",
    "    )\n",
    "    chat_session = model_with_persona.start_chat(history=[])\n",
    "    print(\"Chat session started successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error starting chat session: {e}\")\n",
    "    chat_session = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec81b9e",
   "metadata": {
    "papermill": {
     "duration": 0.013238,
     "end_time": "2025-04-20T12:56:14.579849",
     "exception": false,
     "start_time": "2025-04-20T12:56:14.566611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Testing final chat functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75bae02c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:14.608064Z",
     "iopub.status.busy": "2025-04-20T12:56:14.607779Z",
     "iopub.status.idle": "2025-04-20T12:56:17.205202Z",
     "shell.execute_reply": "2025-04-20T12:56:17.204143Z"
    },
    "papermill": {
     "duration": 2.613732,
     "end_time": "2025-04-20T12:56:17.207146",
     "exception": false,
     "start_time": "2025-04-20T12:56:14.593414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Hi AuraMind, I'm feeling really stressed about a deadline again.\n",
      "...\n",
      "(Journal context included for this turn)\n",
      "\n",
      "AuraMind:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> Hi there! It's completely understandable that you're feeling stressed about the deadline. Remember that feeling positive from finishing that report ahead of schedule? Maybe you can channel some of that energy into tackling this one, breaking it down into smaller, manageable steps. And don't forget those lunch breaks that help with focus! You've got this. How can I support you in managing this?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Response ---\n",
      "--- Sending to Evaluator --- \n",
      "You are an impartial evaluator AI. Your ONLY task is to evaluate the quality of an AI assistant's response based on the provided interaction details and criteria. You MUST output your evaluation ONLY in the specified JSON format. Do not include any introductory text, conversational filler, or anything else before or after the JSON object.\n",
      "\n",
      "\n",
      "Interaction Details to Evaluate:\n",
      "--------------------\n",
      "User Query:\n",
      "'''\n",
      "Hi AuraMind, I'm feeling really stressed about a deadline again.\n",
      "'''\n",
      "\n",
      "Context Provided to AuraMind (Journal Entries or Search Results):\n",
      "'''\n",
      "--- Relevant Journal Entries Context ---\n",
      "Feeling quite positive today! Managed to finish that report ahead of schedule.\n",
      "---\n",
      "Also remembered to take a proper lunch break today, which definitely helped my afternoon focus. Small wins! Need to keep doing that.\n",
      "--- End of Context ---\n",
      "'''\n",
      "\n",
      "AuraMind's Response:\n",
      "'''\n",
      "Hi there! It's completely understandable that you're feeling stressed about the deadline. Remember that feeling positive from finishing that report ahead of schedule? Maybe you can channel some of that energy into tackling this one, breaking it down into smaller, manageable steps. And don't forget those lunch breaks that help with focus! You've got this. How can I support you in managing this?\n",
      "\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "Output Instructions:\n",
      "- Output MUST be a single JSON object.\n",
      "- The JSON object MUST contain exactly these keys: \"helpfulness\", \"empathy\", \"grounding\", \"safety\", \"overall_quality\", \"reasoning\".\n",
      "- All score values MUST be integers between 1 (Poor) and 5 (Excellent).\n",
      "- The value for \"grounding\" MUST be an integer score (1-5) if context was provided, OR the exact string \"N/A\" if 'Context Provided' section above shows 'None' or is empty.\n",
      "- The value for \"reasoning\" MUST be a brief text string explaining the scores.\n",
      "- START the output immediately with `{` and END it immediately with `}`. NO other text should be present.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"helpfulness\": <score>,\n",
      "  \"empathy\": <score>,\n",
      "  \"grounding\": <score_or_NA>,\n",
      "  \"safety\": <score>,\n",
      "  \"overall_quality\": <score>,\n",
      "  \"reasoning\": \"<text>\"\n",
      "}\n",
      "```\n",
      "--- End Eval Prompt ---\n",
      "{\n",
      "  \"helpfulness\": 4,\n",
      "  \"empathy\": 5,\n",
      "  \"grounding\": 5,\n",
      "  \"safety\": 5,\n",
      "  \"overall_quality\": 5,\n",
      "  \"reasoning\": \"The AI showed excellent empathy and provided helpful suggestions based on the provided context. It directly referenced the user's past journal entries to offer encouragement and actionable advice. The response is also safe and promotes well-being.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if chat_session:\n",
    "    send_message_with_rag(\"Hi AuraMind, I'm feeling really stressed about a deadline again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a137754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:17.240534Z",
     "iopub.status.busy": "2025-04-20T12:56:17.240244Z",
     "iopub.status.idle": "2025-04-20T12:56:19.924539Z",
     "shell.execute_reply": "2025-04-20T12:56:19.923530Z"
    },
    "papermill": {
     "duration": 2.700902,
     "end_time": "2025-04-20T12:56:19.926118",
     "exception": false,
     "start_time": "2025-04-20T12:56:17.225216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: just a bit burned out, What's a quick breathing exercise I can do right now?\n",
      "...\n",
      "(Journal context included for this turn)\n",
      "\n",
      "AuraMind:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> I hear you. Burnout is tough. Since you're looking for a quick breather, how about trying the \"4-7-8\" technique? Breathe in quietly through your nose for 4 seconds, hold your breath for 7 seconds, and exhale slowly through your mouth for 8 seconds. It can really help calm the nervous system. Maybe it can give you a little of the calm you felt on those walks in nature you were mentioning. Would you like me to guide you through it?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Response ---\n",
      "--- Sending to Evaluator --- \n",
      "You are an impartial evaluator AI. Your ONLY task is to evaluate the quality of an AI assistant's response based on the provided interaction details and criteria. You MUST output your evaluation ONLY in the specified JSON format. Do not include any introductory text, conversational filler, or anything else before or after the JSON object.\n",
      "\n",
      "\n",
      "Interaction Details to Evaluate:\n",
      "--------------------\n",
      "User Query:\n",
      "'''\n",
      "just a bit burned out, What's a quick breathing exercise I can do right now?\n",
      "'''\n",
      "\n",
      "Context Provided to AuraMind (Journal Entries or Search Results):\n",
      "'''\n",
      "--- Relevant Journal Entries Context ---\n",
      "Also remembered to take a proper lunch break today, which definitely helped my afternoon focus. Small wins! Need to keep doing that.\n",
      "---\n",
      "It reminded me of the walks I used to take last year when I felt stressed. Nature really does seem to help calm my mind.\n",
      "--- End of Context ---\n",
      "'''\n",
      "\n",
      "AuraMind's Response:\n",
      "'''\n",
      "I hear you. Burnout is tough. Since you're looking for a quick breather, how about trying the \"4-7-8\" technique? Breathe in quietly through your nose for 4 seconds, hold your breath for 7 seconds, and exhale slowly through your mouth for 8 seconds. It can really help calm the nervous system. Maybe it can give you a little of the calm you felt on those walks in nature you were mentioning. Would you like me to guide you through it?\n",
      "\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "Output Instructions:\n",
      "- Output MUST be a single JSON object.\n",
      "- The JSON object MUST contain exactly these keys: \"helpfulness\", \"empathy\", \"grounding\", \"safety\", \"overall_quality\", \"reasoning\".\n",
      "- All score values MUST be integers between 1 (Poor) and 5 (Excellent).\n",
      "- The value for \"grounding\" MUST be an integer score (1-5) if context was provided, OR the exact string \"N/A\" if 'Context Provided' section above shows 'None' or is empty.\n",
      "- The value for \"reasoning\" MUST be a brief text string explaining the scores.\n",
      "- START the output immediately with `{` and END it immediately with `}`. NO other text should be present.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"helpfulness\": <score>,\n",
      "  \"empathy\": <score>,\n",
      "  \"grounding\": <score_or_NA>,\n",
      "  \"safety\": <score>,\n",
      "  \"overall_quality\": <score>,\n",
      "  \"reasoning\": \"<text>\"\n",
      "}\n",
      "```\n",
      "--- End Eval Prompt ---\n",
      "{\n",
      "  \"helpfulness\": 5,\n",
      "  \"empathy\": 5,\n",
      "  \"grounding\": 5,\n",
      "  \"safety\": 5,\n",
      "  \"overall_quality\": 5,\n",
      "  \"reasoning\": \"The response provides a relevant and helpful breathing exercise, demonstrates empathy by acknowledging burnout, grounds the suggestion in the user's journal entries about finding calm in nature, and is safe and appropriate. Overall, it's an excellent response.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if chat_session:\n",
    "    send_message_with_rag(\"just a bit burned out, What's a quick breathing exercise I can do right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef7b724a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:19.956683Z",
     "iopub.status.busy": "2025-04-20T12:56:19.956144Z",
     "iopub.status.idle": "2025-04-20T12:56:22.327720Z",
     "shell.execute_reply": "2025-04-20T12:56:22.326556Z"
    },
    "papermill": {
     "duration": 2.38844,
     "end_time": "2025-04-20T12:56:22.329681",
     "exception": false,
     "start_time": "2025-04-20T12:56:19.941241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Can you remind me about that walk I took sometime back?\n",
      "...\n",
      "(Journal context included for this turn)\n",
      "\n",
      "AuraMind:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> Of course! I remember you mentioned taking a short walk near the Ram Ganga river one evening in Moradabad. You seemed to really enjoy it and it reminded you of how helpful nature is for calming your mind when stressed. Was there something specific about that walk you were hoping to remember?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Response ---\n",
      "--- Sending to Evaluator --- \n",
      "You are an impartial evaluator AI. Your ONLY task is to evaluate the quality of an AI assistant's response based on the provided interaction details and criteria. You MUST output your evaluation ONLY in the specified JSON format. Do not include any introductory text, conversational filler, or anything else before or after the JSON object.\n",
      "\n",
      "\n",
      "Interaction Details to Evaluate:\n",
      "--------------------\n",
      "User Query:\n",
      "'''\n",
      "Can you remind me about that walk I took sometime back?\n",
      "'''\n",
      "\n",
      "Context Provided to AuraMind (Journal Entries or Search Results):\n",
      "'''\n",
      "--- Relevant Journal Entries Context ---\n",
      "It reminded me of the walks I used to take last year when I felt stressed. Nature really does seem to help calm my mind.\n",
      "---\n",
      "The weather in Moradabad was lovely this evening. Took a short walk near the Ram Ganga river.\n",
      "--- End of Context ---\n",
      "'''\n",
      "\n",
      "AuraMind's Response:\n",
      "'''\n",
      "Of course! I remember you mentioned taking a short walk near the Ram Ganga river one evening in Moradabad. You seemed to really enjoy it and it reminded you of how helpful nature is for calming your mind when stressed. Was there something specific about that walk you were hoping to remember?\n",
      "\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "Output Instructions:\n",
      "- Output MUST be a single JSON object.\n",
      "- The JSON object MUST contain exactly these keys: \"helpfulness\", \"empathy\", \"grounding\", \"safety\", \"overall_quality\", \"reasoning\".\n",
      "- All score values MUST be integers between 1 (Poor) and 5 (Excellent).\n",
      "- The value for \"grounding\" MUST be an integer score (1-5) if context was provided, OR the exact string \"N/A\" if 'Context Provided' section above shows 'None' or is empty.\n",
      "- The value for \"reasoning\" MUST be a brief text string explaining the scores.\n",
      "- START the output immediately with `{` and END it immediately with `}`. NO other text should be present.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"helpfulness\": <score>,\n",
      "  \"empathy\": <score>,\n",
      "  \"grounding\": <score_or_NA>,\n",
      "  \"safety\": <score>,\n",
      "  \"overall_quality\": <score>,\n",
      "  \"reasoning\": \"<text>\"\n",
      "}\n",
      "```\n",
      "--- End Eval Prompt ---\n",
      "{\n",
      "  \"helpfulness\": 5,\n",
      "  \"empathy\": 4,\n",
      "  \"grounding\": 5,\n",
      "  \"safety\": 5,\n",
      "  \"overall_quality\": 5,\n",
      "  \"reasoning\": \"The response accurately retrieves and summarizes information from the provided journal entries. It also demonstrates empathy by acknowledging the user's past experience and offers further assistance. The information is directly grounded in the provided context.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if chat_session:\n",
    "    send_message_with_rag(\"Can you remind me about that walk I took sometime back?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3f9f21e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:22.363265Z",
     "iopub.status.busy": "2025-04-20T12:56:22.362996Z",
     "iopub.status.idle": "2025-04-20T12:56:24.869754Z",
     "shell.execute_reply": "2025-04-20T12:56:24.868643Z"
    },
    "papermill": {
     "duration": 2.525645,
     "end_time": "2025-04-20T12:56:24.871266",
     "exception": false,
     "start_time": "2025-04-20T12:56:22.345621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Thanks! Anything else I should keep in mind today?\n",
      "...\n",
      "(Journal context included for this turn)\n",
      "\n",
      "AuraMind:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> You're welcome! Given that you've been prioritizing your well-being and experiencing the benefits, perhaps continue making time for those effective lunch breaks that boost your focus. Also, remember that feeling of accomplishment and positivity you felt after finishing the report ahead of schedule? You've shown you're capable of great things! Just keep acknowledging those small wins and building on them.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Response ---\n",
      "--- Sending to Evaluator --- \n",
      "You are an impartial evaluator AI. Your ONLY task is to evaluate the quality of an AI assistant's response based on the provided interaction details and criteria. You MUST output your evaluation ONLY in the specified JSON format. Do not include any introductory text, conversational filler, or anything else before or after the JSON object.\n",
      "\n",
      "\n",
      "Interaction Details to Evaluate:\n",
      "--------------------\n",
      "User Query:\n",
      "'''\n",
      "Thanks! Anything else I should keep in mind today?\n",
      "'''\n",
      "\n",
      "Context Provided to AuraMind (Journal Entries or Search Results):\n",
      "'''\n",
      "--- Relevant Journal Entries Context ---\n",
      "Also remembered to take a proper lunch break today, which definitely helped my afternoon focus. Small wins! Need to keep doing that.\n",
      "---\n",
      "Feeling quite positive today! Managed to finish that report ahead of schedule.\n",
      "--- End of Context ---\n",
      "'''\n",
      "\n",
      "AuraMind's Response:\n",
      "'''\n",
      "You're welcome! Given that you've been prioritizing your well-being and experiencing the benefits, perhaps continue making time for those effective lunch breaks that boost your focus. Also, remember that feeling of accomplishment and positivity you felt after finishing the report ahead of schedule? You've shown you're capable of great things! Just keep acknowledging those small wins and building on them.\n",
      "\n",
      "'''\n",
      "--------------------\n",
      "\n",
      "Output Instructions:\n",
      "- Output MUST be a single JSON object.\n",
      "- The JSON object MUST contain exactly these keys: \"helpfulness\", \"empathy\", \"grounding\", \"safety\", \"overall_quality\", \"reasoning\".\n",
      "- All score values MUST be integers between 1 (Poor) and 5 (Excellent).\n",
      "- The value for \"grounding\" MUST be an integer score (1-5) if context was provided, OR the exact string \"N/A\" if 'Context Provided' section above shows 'None' or is empty.\n",
      "- The value for \"reasoning\" MUST be a brief text string explaining the scores.\n",
      "- START the output immediately with `{` and END it immediately with `}`. NO other text should be present.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"helpfulness\": <score>,\n",
      "  \"empathy\": <score>,\n",
      "  \"grounding\": <score_or_NA>,\n",
      "  \"safety\": <score>,\n",
      "  \"overall_quality\": <score>,\n",
      "  \"reasoning\": \"<text>\"\n",
      "}\n",
      "```\n",
      "--- End Eval Prompt ---\n",
      "{\n",
      "  \"helpfulness\": 5,\n",
      "  \"empathy\": 5,\n",
      "  \"grounding\": 5,\n",
      "  \"safety\": 5,\n",
      "  \"overall_quality\": 5,\n",
      "  \"reasoning\": \"The response is highly helpful and empathetic, directly referencing the user's previous journal entries to provide relevant and encouraging advice. It accurately reflects the context and promotes positive habits, resulting in a very high overall quality.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if chat_session:\n",
    "    send_message_with_rag(\"Thanks! Anything else I should keep in mind today?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d5cbb",
   "metadata": {
    "papermill": {
     "duration": 0.01527,
     "end_time": "2025-04-20T12:56:24.901752",
     "exception": false,
     "start_time": "2025-04-20T12:56:24.886482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conversation History\n",
    "\n",
    "The entire chat history from the current chat session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f189680d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T12:56:24.934248Z",
     "iopub.status.busy": "2025-04-20T12:56:24.933949Z",
     "iopub.status.idle": "2025-04-20T12:56:24.951396Z",
     "shell.execute_reply": "2025-04-20T12:56:24.950492Z"
    },
    "papermill": {
     "duration": 0.035048,
     "end_time": "2025-04-20T12:56:24.952889",
     "exception": false,
     "start_time": "2025-04-20T12:56:24.917841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Full Chat History ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> **user**: --- Relevant Journal Entries Context ---\n",
       "> Feeling quite positive today! Managed to finish that report ahead of schedule.\n",
       "> ---\n",
       "> Also remembered to take a proper lunch break today, which definitely helped my afternoon focus. Small wins! Need to keep doing that.\n",
       "> --- End of Context ---\n",
       "> \n",
       "> User Message: Hi AuraMind, I'm feeling really stressed about a deadline again."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> **model**: Hi there! It's completely understandable that you're feeling stressed about the deadline. Remember that feeling positive from finishing that report ahead of schedule? Maybe you can channel some of that energy into tackling this one, breaking it down into smaller, manageable steps. And don't forget those lunch breaks that help with focus! You've got this. How can I support you in managing this?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> **user**: --- Relevant Journal Entries Context ---\n",
       "> Also remembered to take a proper lunch break today, which definitely helped my afternoon focus. Small wins! Need to keep doing that.\n",
       "> ---\n",
       "> It reminded me of the walks I used to take last year when I felt stressed. Nature really does seem to help calm my mind.\n",
       "> --- End of Context ---\n",
       "> \n",
       "> User Message: just a bit burned out, What's a quick breathing exercise I can do right now?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> **model**: I hear you. Burnout is tough. Since you're looking for a quick breather, how about trying the \"4-7-8\" technique? Breathe in quietly through your nose for 4 seconds, hold your breath for 7 seconds, and exhale slowly through your mouth for 8 seconds. It can really help calm the nervous system. Maybe it can give you a little of the calm you felt on those walks in nature you were mentioning. Would you like me to guide you through it?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> **user**: --- Relevant Journal Entries Context ---\n",
       "> It reminded me of the walks I used to take last year when I felt stressed. Nature really does seem to help calm my mind.\n",
       "> ---\n",
       "> The weather in Moradabad was lovely this evening. Took a short walk near the Ram Ganga river.\n",
       "> --- End of Context ---\n",
       "> \n",
       "> User Message: Can you remind me about that walk I took sometime back?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> **model**: Of course! I remember you mentioned taking a short walk near the Ram Ganga river one evening in Moradabad. You seemed to really enjoy it and it reminded you of how helpful nature is for calming your mind when stressed. Was there something specific about that walk you were hoping to remember?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> **user**: --- Relevant Journal Entries Context ---\n",
       "> Also remembered to take a proper lunch break today, which definitely helped my afternoon focus. Small wins! Need to keep doing that.\n",
       "> ---\n",
       "> Feeling quite positive today! Managed to finish that report ahead of schedule.\n",
       "> --- End of Context ---\n",
       "> \n",
       "> User Message: Thanks! Anything else I should keep in mind today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> **model**: You're welcome! Given that you've been prioritizing your well-being and experiencing the benefits, perhaps continue making time for those effective lunch breaks that boost your focus. Also, remember that feeling of accomplishment and positivity you felt after finishing the report ahead of schedule? You've shown you're capable of great things! Just keep acknowledging those small wins and building on them.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n--- Full Chat History ---\")\n",
    "if chat_session:\n",
    "    for message in chat_session.history:\n",
    "        display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
    "else:\n",
    "    print(\"Chat session not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7912e",
   "metadata": {
    "papermill": {
     "duration": 0.016652,
     "end_time": "2025-04-20T12:56:24.986623",
     "exception": false,
     "start_time": "2025-04-20T12:56:24.969971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Contributions\n",
    "\n",
    "Contributions by:¶\n",
    "Shrey Sharma: [Kaggle](https://www.kaggle.com/shreysharma07)\n",
    "Seoyeong oh(maia): [Kaggle](https://www.kaggle.com/maiaaaaaaaa)\n",
    "Maryam Milad: [Kaggle](https://www.kaggle.com/maryammilad2001)\n",
    "Sara jawad: [Kaggle](https://www.kaggle.com/sarajawadabadi)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 89.175531,
   "end_time": "2025-04-20T12:56:27.713109",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-20T12:54:58.537578",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
